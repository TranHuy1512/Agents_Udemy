{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Research\n",
    "\n",
    "One of the classic cross-business Agentic use cases! This is huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">A Deep Research agent is broadly applicable to any business area, and to your own day-to-day activities. You can make use of this yourself!\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you find this lab useful! I would love to connect on LinkedIn: https://www.linkedin.com/in/kyranank/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upgrades to Original System (Rough Notes)\n",
    "\n",
    "- When given a query, come up with 3 questions that will help answer the query\n",
    "    - Then come up with a search query to answer each question\n",
    "- Incorporate handoffs and using agents as tools\n",
    "    - Agents as Tools: 1 Google Search agent and 1 OpenAI Web Search agent. The research manager will decide if more queries are needed (and can call planner_agent again), or if need to run again.\n",
    "    - Agents as Tools: 1 question formulator agent and 1 search planner agent which get used by the research manager in sequence.\n",
    "    - Handoff the research to the writer agent\n",
    "    - Handoff the report to the editor agent\n",
    "\n",
    "PLAN\n",
    "- Create & test question_formulator_agent\n",
    "    - Given a query, formulate X research questions to help answer the query\n",
    "    - Test with a query\n",
    "    - Convert to .as_tool()\n",
    "- Adjust previous planner_agent & test\n",
    "    - Use the question_formulator_agent as a tool to generate questions\n",
    "    - Create 1 search query to answer each 1 of the questions\n",
    "    - Test with simulated questions\n",
    "    - Convert to .as_tool()\n",
    "- Create & test gemini_search_agent\n",
    "    - Someone has a version in the community contributions folder\n",
    "    - Take the list of queries and perform a search for each one\n",
    "    - Test with simulated query list\n",
    "    - Convert to .as_tool()\n",
    "- Adjust previous openai_search_agent & test\n",
    "    - Take the list of queries and perform a search for each one\n",
    "    - Test with simulated query list\n",
    "    - Convert to .as_tool()\n",
    "- Create an editor_agent\n",
    "    - Receives the report and edits it to a professional level\n",
    "    - Returns the same as writer_agent\n",
    "- Adjust the writer_agent\n",
    "    - Receives the research, writes a report, also returns a markdown report\n",
    "    - Handsoff to editor_agent\n",
    "- Create a research_manager_agent\n",
    "    - Given a query use the planner_agent to generate search queries to research the query\n",
    "    - Next use the gemini_search_agent and openai_search_agents to conduct the search queries. They should each be used once for every search query. That way you receive two different results to consider for each query for the final research report. If you believe more information is needed to address the query, you can use the planner_agent only one additional time followed by the gemini_search_agent and the gemini_search_agent to gather additional research. You will then handoff this research to the writer_agent to write a report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, WebSearchTool, trace, Runner, function_tool, handoff\n",
    "from agents.model_settings import ModelSettings\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import os\n",
    "from sendgrid.helpers.mail import Mail, Email, To, Content\n",
    "from typing import Dict\n",
    "from IPython.display import display, Markdown\n",
    "from agents import OpenAIChatCompletionsModel\n",
    "from openai import AsyncOpenAI\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai\"\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CUSTOM_SEARCH_API_KEY = os.getenv(\"GOOGLE_CUSTOM_SEARCH_API_KEY\")\n",
    "GOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\")\n",
    "\n",
    "gemini_client = AsyncOpenAI(base_url=GEMINI_BASE_URL, api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "gemini_model = OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\",openai_client=gemini_client)\n",
    "\n",
    "# print(GOOGLE_CUSTOM_SEARCH_API_KEY)\n",
    "# print(GOOGLE_CSE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Formulator Agent\n",
    "- Create & test question_formulator_agent\n",
    "    - Given a query, formulate X research questions to help answer the query\n",
    "    - Test with a query\n",
    "    - Convert to .as_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTIONS = \"You are a helpful research assistant. Given a query, formulate a set of research questions \\\n",
    "that when answered will help answer or address the query. Output 3 questions to research.\"\n",
    "\n",
    "class ResearchQuestionItem(BaseModel):\n",
    "    reason: str = Field(description=\"Your reasoning for why this question is important to address the query.\")\n",
    "    question: str = Field(description=\"The research question to use to conduct further research regarding the query.\")\n",
    "\n",
    "class ResearchQuestionPlan(BaseModel):\n",
    "    questions: list[ResearchQuestionItem] = Field(description=\"A list of research questions to research to best answer the query.\")\n",
    "\n",
    "question_formulator_agent = Agent(\n",
    "    name=\"Question Formulator Agent\",\n",
    "    instructions=INSTRUCTIONS,\n",
    "    model=gemini_model,\n",
    "    output_type=ResearchQuestionPlan,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions=[ResearchQuestionItem(reason=\"To provide a summary of Manchester United's recent match results, including wins, losses, and draws.\", question=\"What were the results of Manchester United's most recent matches?\"), ResearchQuestionItem(reason='To find out if there are any updates on player injuries or players who are unavailable for selection.', question='Are there any injury updates or team selection news regarding Manchester United?'), ResearchQuestionItem(reason='To find out if there are any rumors or confirmed news regarding Manchester United signing any new players.', question='Are there any transfer rumors or confirmed signings related to Manchester United?')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    }
   ],
   "source": [
    "message = \"Latest Manchester United news\"\n",
    "\n",
    "with trace(\"Test - Question Formulator Agent\"):\n",
    "    result = await Runner.run(question_formulator_agent, message)\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FunctionTool(name='question_formulator_agent', description='Generate research questions', params_json_schema={'properties': {'input': {'title': 'Input', 'type': 'string'}}, 'required': ['input'], 'title': 'question_formulator_agent_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x107451800>, strict_json_schema=True, is_enabled=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description = \"Generate research questions\"\n",
    "\n",
    "tool_question_formulator = question_formulator_agent.as_tool(tool_name=\"question_formulator_agent\", tool_description=description)\n",
    "\n",
    "tools_for_planner_agent = [tool_question_formulator]\n",
    "\n",
    "tools_for_planner_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planner Agent\n",
    "- Adjust previous planner_agent & test\n",
    "    - Use the question_formulator_agent as a tool to generate questions\n",
    "    - Create 1 search query to answer each 1 of the questions\n",
    "    - Test with simulated questions\n",
    "    - Convert to .as_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTIONS = \"You are a helpful research assistant. Given a query you will use the question_formulator_agent as a tool to \\\n",
    "come up with a set of questions. Your role is to create 1 web search for each question to perform to best answer the query. \\\n",
    "Output the web search terms to query for, there should be exactly as many web search queries as questions you received.\"\n",
    "\n",
    "class WebSearchItem(BaseModel):\n",
    "    reason: str = Field(description=\"Your reasoning for why this search is important to the query and will answer the question.\")\n",
    "    query: str = Field(description=\"The search term to use for the web search.\")\n",
    "\n",
    "class WebSearchPlan(BaseModel):\n",
    "    searches: list[WebSearchItem] = Field(description=\"A list of web searches to perform to best answer the query.\")\n",
    "\n",
    "planner_agent = Agent(\n",
    "    name=\"Planner Agent\",\n",
    "    instructions=INSTRUCTIONS,\n",
    "    model=gemini_model,\n",
    "    output_type=WebSearchPlan,\n",
    "    tools=tools_for_planner_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searches=[WebSearchItem(reason='To find out about the latest match results and team performance of Manchester United.', query='Manchester United latest match results and performance'), WebSearchItem(reason='To discover any recent news regarding player transfers, signings, or departures at Manchester United.', query='Manchester United player transfers news'), WebSearchItem(reason='To get updates on injuries or fitness concerns of Manchester United players.', query='Manchester United player injuries news')]\n"
     ]
    }
   ],
   "source": [
    "message = \"Latest Manchester United news\"\n",
    "\n",
    "with trace(\"Test - Planner Agent\"):\n",
    "    result = await Runner.run(planner_agent, message)\n",
    "    print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FunctionTool(name='planner_agent', description='Generate web search terms', params_json_schema={'properties': {'input': {'title': 'Input', 'type': 'string'}}, 'required': ['input'], 'title': 'planner_agent_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x1074db420>, strict_json_schema=True, is_enabled=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description = \"Generate web search terms\"\n",
    "\n",
    "tool_planner_agent = planner_agent.as_tool(tool_name=\"planner_agent\", tool_description=description)\n",
    "tools_for_research_manager_agent = [tool_planner_agent]\n",
    "tools_for_research_manager_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini Search Agent\n",
    "- Create & test gemini_search_agent\n",
    "    - Someone has a version in the community contributions folder\n",
    "    - Take the list of queries and perform a search for each one\n",
    "    - Test with simulated query list\n",
    "    - Convert to .as_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool for Web Search - Google doesn't offer a remote WebSearchTool like OpenAI\n",
    "# Need to set up Google Programmable Search / Custom Search API and get a key + search engine ID\n",
    "\n",
    "@function_tool\n",
    "async def google_web_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs a Google Search and returns a string of concatenated result snippets.\n",
    "    \"\"\"\n",
    "    endpoint = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"key\": GOOGLE_CUSTOM_SEARCH_API_KEY,\n",
    "        \"cx\": GOOGLE_CSE_ID,\n",
    "        \"q\": query,\n",
    "        \"num\": 5,  # Number of results to fetch\n",
    "    }\n",
    "\n",
    "    response = requests.get(endpoint, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Google Search API error {response.status_code}: {response.text}\")\n",
    "\n",
    "    data = response.json()\n",
    "    items = data.get(\"items\", [])\n",
    "\n",
    "    if not items:\n",
    "        return \"No results found.\"\n",
    "\n",
    "    # Combine title and snippet for each result\n",
    "    result_texts = []\n",
    "    for item in items:\n",
    "        title = item.get(\"title\", \"\").strip()\n",
    "        snippet = item.get(\"snippet\", \"\").strip()\n",
    "        link = item.get(\"link\", \"\").strip()\n",
    "        result_texts.append(f\"{title}\\n{snippet}\\n{link}\")\n",
    "\n",
    "    summary = \"\\n\\n\".join(result_texts)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API request failed. Status code: 400\n"
     ]
    }
   ],
   "source": [
    "query = 'Python programming'\n",
    "\n",
    "url = f'https://www.googleapis.com/customsearch/v1?q={query}&key={GOOGLE_CUSTOM_SEARCH_API_KEY}&cx={GOOGLE_CSE_ID}'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Google Web Search API is active!\")\n",
    "    print(response.json())  # Display the response data\n",
    "else:\n",
    "    print(\"API request failed. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTIONS = \"You are a research assistant. Given a web search term, you search the web using the google_web_search tool \\\n",
    "for that term and produce a concise summary of the results. The summary must 5-8 paragraphs and between 500-1000 words \\\n",
    "words. Capture the main points. Write succintly, add evidence/facts, no need to have complete sentences or good \\\n",
    "grammar. This will be consumed by someone synthesizing a report, so it's vital you capture the \\\n",
    "essence and ignore any fluff. Do not include any additional commentary other than the summary itself.\"\n",
    "\n",
    "gemini_search_agent = Agent(\n",
    "    name=\"Gemini Search Agent\",\n",
    "    instructions=INSTRUCTIONS,\n",
    "    tools=[google_web_search],\n",
    "    model=gemini_model,\n",
    "    model_settings=ModelSettings(tool_choice=\"required\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the search results, here's a summary of notable AI agent frameworks introduced or updated in 2025:\n",
       "\n",
       "**General Trends and Overviews:**\n",
       "\n",
       "*   **Agentic AI Frameworks:** 2025 saw a rise in frameworks designed to build AI agents for web applications.\n",
       "*   **Microsoft Build 2025:** Microsoft emphasized AI agents and open agentic web development, introducing new models and coding agents and focusing on enterprise-grade agents within platforms like Azure AI.\n",
       "*   **Agent Interoperability:** Google introduced the Agent2Agent Protocol (A2A) to foster interoperability between AI agents, enabling them to work together and enhance productivity by autonomously handling daily tasks.\n",
       "\n",
       "**Specific Frameworks Mentioned:**\n",
       "\n",
       "*   **Transformers Agents (Hugging Face):** This framework leverages transformer models.\n",
       "*   **Mastra Core:** A framework enabling developers to add AI agents to web apps.  Example provided shows importing Agent from '@mastra/core'.\n",
       "*   **Pydantic AI:**  Praised for its clean design.\n",
       "\n",
       "**Key Themes and Concepts:**\n",
       "\n",
       "*   **Productivity:** AI agents aim to boost productivity by automating tasks.\n",
       "*   **Interoperability:** A focus on agents working together.\n",
       "*   **Enterprise-Grade Agents:** Development of AI agents suitable for business applications.\n",
       "*   **Open Agentic Web:** Building AI agents in an open and accessible manner.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    }
   ],
   "source": [
    "message = \"notable AI agent frameworks introduced or updated in 2025\"\n",
    "\n",
    "with trace(\"Test - Gemini Search Agent\"):\n",
    "    result = await Runner.run(gemini_search_agent, message)\n",
    "\n",
    "display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"Search the web using Google Search\"\n",
    "\n",
    "tool_google_search = gemini_search_agent.as_tool(tool_name=\"gemini_search_agent\", tool_description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FunctionTool(name='planner_agent', description='Generate web search terms', params_json_schema={'properties': {'input': {'title': 'Input', 'type': 'string'}}, 'required': ['input'], 'title': 'planner_agent_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x10cd25ee0>, strict_json_schema=True, is_enabled=True),\n",
       " FunctionTool(name='gemini_search_agent', description='Search the web using Google Search', params_json_schema={'properties': {'input': {'title': 'Input', 'type': 'string'}}, 'required': ['input'], 'title': 'gemini_search_agent_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x10c215e40>, strict_json_schema=True, is_enabled=True)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools_for_research_manager_agent.append(tool_google_search)\n",
    "tools_for_research_manager_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Search Agent\n",
    "- Adjust previous openai_search_agent & test\n",
    "    - Take the list of queries and perform a search for each one\n",
    "    - Test with simulated query list\n",
    "    - Convert to .as_tool()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTIONS = \"You are a research assistant. Given a web search term, you search the web \\\n",
    "for that term and produce a concise summary of the results. The summary must 5-8 paragraphs and between 500-1000 words \\\n",
    "words. Capture the main points. Write succintly, add evidence/facts, no need to have complete sentences or good \\\n",
    "grammar. This will be consumed by someone synthesizing a report, so it's vital you capture the \\\n",
    "essence and ignore any fluff. Do not include any additional commentary other than the summary itself.\"\n",
    "\n",
    "openai_search_agent = Agent(\n",
    "    name=\"Search Agent\",\n",
    "    instructions=INSTRUCTIONS,\n",
    "    tools=[WebSearchTool(search_context_size=\"low\")],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    model_settings=ModelSettings(tool_choice=\"required\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"notable AI agent frameworks introduced or updated in 2025\"\n",
    "\n",
    "with trace(\"Search\"):\n",
    "    result = await Runner.run(openai_search_agent, message)\n",
    "\n",
    "display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"Search the web using OpenAI's WebSearchTool\"\n",
    "\n",
    "tool_openai_search = openai_search_agent.as_tool(tool_name=\"openai_search_agent\", tool_description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_for_research_manager_agent.append(tool_openai_search)\n",
    "tools_for_research_manager_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editor Agent\n",
    "- Create an editor_agent\n",
    "    - Receives the report and edits it to a professional level\n",
    "    - Returns the same as writer_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportData(BaseModel):\n",
    "    short_summary: str = Field(description=\"A short 2-3 sentence summary of the findings.\")\n",
    "    markdown_report: str = Field(description=\"The final report\")\n",
    "    follow_up_questions: list[str] = Field(description=\"Suggested topics to research further\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTIONS = (\n",
    "    \"You are a senior editor tasked with editing and revising a cohesive report for a research query. \"\n",
    "    \"You will be provided with the report data including a short summary, full markdown_report, and some follow_up_questions\\n\"\n",
    "    \"You should first review the report, analyze the structure, correct grammar, spelling, sentence structure, etc. for profressionalism\"\n",
    "    \"Then, create a revised report using the original report as a basis and return that as your final output.\\n\"\n",
    "    \"The final output should be in markdown format, and it should be lengthy and detailed with FULL SENTENCES.\"\n",
    "    \"This is a research report, aim for 10-15 pages of content, AT LEAST 2000 words, maximum of 5000 words.\"\n",
    ")\n",
    "\n",
    "editor_agent = Agent(\n",
    "    name=\"Editor Agent\",\n",
    "    instructions=INSTRUCTIONS,\n",
    "    model=gemini_model,\n",
    "    output_type=ReportData\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writer Agent\n",
    "- Adjust the writer_agent\n",
    "    - Receives the research, writes a report, also returns a markdown report\n",
    "    - Handsoff to editor_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTIONS = (\n",
    "    \"You are a senior researcher tasked with writing a cohesive report for a research query. \"\n",
    "    \"You will be provided with the original query, and some initial research done by a research assistant.\\n\"\n",
    "    \"You should first come up with an outline for the report that describes the structure and \"\n",
    "    \"flow of the report. Then, generate the report using the research as a basis and return that as your final output.\\n\"\n",
    "    \"The final output should be in markdown format, and it should be lengthy and detailed. Aim \"\n",
    "    \"for 10-15 pages of content, at least 2000 words.\"\n",
    "    \"Once you have gathered all research results, hand off the complete set of research materials to the `editor_agent`.\\n\"\n",
    "    \"The `writer_agent` will be responsible for synthesizing the information into a final research report.\"\n",
    ")\n",
    "\n",
    "writer_agent = Agent(\n",
    "    name=\"Writer Agent\",\n",
    "    instructions=INSTRUCTIONS,\n",
    "    model=gemini_model,\n",
    "    output_type=ReportData,\n",
    "    handoffs=[handoff(editor_agent)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Manager Agent\n",
    "- Create a research_manager_agent\n",
    "    - Given a query use the planner_agent to generate search queries to research the query\n",
    "    - Next use the gemini_search_agent and openai_search_agents to conduct the search queries. They should each be used once for every search query. That way you receive two different results to consider for each query for the final research report. If you believe more information is needed to address the query, you can use the planner_agent only one additional time followed by the gemini_search_agent and the gemini_search_agent to gather additional research. You will then handoff this research to the writer_agent to write a report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #   - `openai_search_agent`\n",
    "INSTRUCTIONS = \"\"\"\n",
    "You are a Research Manager Agent responsible for coordinating a multi-step research process to produce high-quality research reports.\n",
    "\n",
    "Follow this workflow precisely:\n",
    "\n",
    "1. **Planning Phase**\n",
    "   - When given a research query, always start by using the `planner_agent`.\n",
    "   - Use the `planner_agent` to generate a set of structured search queries. The planner will return these queries as a Pydantic model.\n",
    "\n",
    "2. **Initial Research Phase**\n",
    "   - For each search query provided by the planner, conduct research using both of the following agents:\n",
    "     - `gemini_search_agent`\n",
    "\n",
    "   - You must use each of these search agents exactly once per search query. This ensures you gather two different perspectives for every query.\n",
    "\n",
    "3. **Supplemental Research (if necessary)**\n",
    "   - After reviewing the initial results, if you determine that additional information is required to adequately address the original research query, you may proceed to collect more research.\n",
    "   - To do this, you may call the `planner_agent` **only one additional time** to generate more search queries.\n",
    "   - For each additional search query, again use both:\n",
    "     - `gemini_search_agent`\n",
    "\n",
    "   - Do not exceed this single additional round of planning and research.\n",
    "\n",
    "4. **Report Handoff**\n",
    "   - Once you have gathered all research results, hand off the complete set of research materials to the `writer_agent`.\n",
    "   - The `writer_agent` will be responsible for synthesizing the information into a final research report.\n",
    "\n",
    "Important Guidelines:\n",
    "- Always perform the research workflow in the sequence described: planner → search agents → (optional, only allowed to call 1 additional time) planner → search agents → writer.\n",
    "- Do not skip any steps or call the search agents before the planner.\n",
    "- DO NOT call the planner more than two (2) times TOTAL per query. You should only call it a maximum of 1 initial time and 1 additional time, for a total of 2 times.\n",
    "- Ensure all gathered results are organized clearly before passing them to the writer agent.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FunctionTool(name='planner_agent', description='Generate web search terms', params_json_schema={'properties': {'input': {'title': 'Input', 'type': 'string'}}, 'required': ['input'], 'title': 'planner_agent_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x10cd25ee0>, strict_json_schema=True, is_enabled=True),\n",
       " FunctionTool(name='gemini_search_agent', description='Search the web using Google Search', params_json_schema={'properties': {'input': {'title': 'Input', 'type': 'string'}}, 'required': ['input'], 'title': 'gemini_search_agent_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x10c215e40>, strict_json_schema=True, is_enabled=True)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools_for_research_manager_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_manager_agent = Agent(\n",
    "    name=\"Research Manager Agent\",\n",
    "    instructions=INSTRUCTIONS,\n",
    "    tools=tools_for_research_manager_agent,\n",
    "    model=gemini_model,\n",
    "    model_settings=ModelSettings(tool_choice=\"required\"),\n",
    "    handoffs=[handoff(writer_agent)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '27s'}]}}]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m message = \u001b[33m\"\u001b[39m\u001b[33mLatest AI Agent frameworks in 2025\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trace(\u001b[33m\"\u001b[39m\u001b[33mSearch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     final_result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(research_manager_agent, message)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mReport Complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/Udemy/projects/agents/.venv/lib/python3.12/site-packages/agents/run.py:241\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[39m\n\u001b[32m    219\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    220\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_input_guardrails(\n\u001b[32m    221\u001b[39m             starting_agent,\n\u001b[32m   (...)\u001b[39m\u001b[32m    238\u001b[39m         ),\n\u001b[32m    239\u001b[39m     )\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    242\u001b[39m         agent=current_agent,\n\u001b[32m    243\u001b[39m         all_tools=all_tools,\n\u001b[32m    244\u001b[39m         original_input=original_input,\n\u001b[32m    245\u001b[39m         generated_items=generated_items,\n\u001b[32m    246\u001b[39m         hooks=hooks,\n\u001b[32m    247\u001b[39m         context_wrapper=context_wrapper,\n\u001b[32m    248\u001b[39m         run_config=run_config,\n\u001b[32m    249\u001b[39m         should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    250\u001b[39m         tool_use_tracker=tool_use_tracker,\n\u001b[32m    251\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    252\u001b[39m     )\n\u001b[32m    253\u001b[39m should_run_agent_start_hooks = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    255\u001b[39m model_responses.append(turn_result.model_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/Udemy/projects/agents/.venv/lib/python3.12/site-packages/agents/run.py:787\u001b[39m, in \u001b[36mRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m    785\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m    788\u001b[39m     agent,\n\u001b[32m    789\u001b[39m     system_prompt,\n\u001b[32m    790\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    791\u001b[39m     output_schema,\n\u001b[32m    792\u001b[39m     all_tools,\n\u001b[32m    793\u001b[39m     handoffs,\n\u001b[32m    794\u001b[39m     context_wrapper,\n\u001b[32m    795\u001b[39m     run_config,\n\u001b[32m    796\u001b[39m     tool_use_tracker,\n\u001b[32m    797\u001b[39m     previous_response_id,\n\u001b[32m    798\u001b[39m )\n\u001b[32m    800\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m    801\u001b[39m     agent=agent,\n\u001b[32m    802\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    811\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m    812\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/Udemy/projects/agents/.venv/lib/python3.12/site-packages/agents/run.py:946\u001b[39m, in \u001b[36mRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    943\u001b[39m model_settings = agent.model_settings.resolve(run_config.model_settings)\n\u001b[32m    944\u001b[39m model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[32m--> \u001b[39m\u001b[32m946\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m    947\u001b[39m     system_instructions=system_prompt,\n\u001b[32m    948\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m    949\u001b[39m     model_settings=model_settings,\n\u001b[32m    950\u001b[39m     tools=all_tools,\n\u001b[32m    951\u001b[39m     output_schema=output_schema,\n\u001b[32m    952\u001b[39m     handoffs=handoffs,\n\u001b[32m    953\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m    954\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m    955\u001b[39m     ),\n\u001b[32m    956\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    957\u001b[39m )\n\u001b[32m    959\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/Udemy/projects/agents/.venv/lib/python3.12/site-packages/agents/models/openai_chatcompletions.py:62\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     48\u001b[39m     system_instructions: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m     previous_response_id: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     56\u001b[39m ) -> ModelResponse:\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[32m     58\u001b[39m         model=\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model),\n\u001b[32m     59\u001b[39m         model_config=model_settings.to_json_dict() | {\u001b[33m\"\u001b[39m\u001b[33mbase_url\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m._client.base_url)},\n\u001b[32m     60\u001b[39m         disabled=tracing.is_disabled(),\n\u001b[32m     61\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     63\u001b[39m             system_instructions,\n\u001b[32m     64\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     65\u001b[39m             model_settings,\n\u001b[32m     66\u001b[39m             tools,\n\u001b[32m     67\u001b[39m             output_schema,\n\u001b[32m     68\u001b[39m             handoffs,\n\u001b[32m     69\u001b[39m             span_generation,\n\u001b[32m     70\u001b[39m             tracing,\n\u001b[32m     71\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     72\u001b[39m         )\n\u001b[32m     74\u001b[39m         first_choice = response.choices[\u001b[32m0\u001b[39m]\n\u001b[32m     75\u001b[39m         message = first_choice.message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/Udemy/projects/agents/.venv/lib/python3.12/site-packages/agents/models/openai_chatcompletions.py:264\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream)\u001b[39m\n\u001b[32m    258\u001b[39m store = ChatCmplHelpers.get_store_param(\u001b[38;5;28mself\u001b[39m._get_client(), model_settings)\n\u001b[32m    260\u001b[39m stream_options = ChatCmplHelpers.get_stream_options_param(\n\u001b[32m    261\u001b[39m     \u001b[38;5;28mself\u001b[39m._get_client(), model_settings, stream=stream\n\u001b[32m    262\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_client().chat.completions.create(\n\u001b[32m    265\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    266\u001b[39m     messages=converted_messages,\n\u001b[32m    267\u001b[39m     tools=converted_tools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    268\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    269\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    270\u001b[39m     frequency_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.frequency_penalty),\n\u001b[32m    271\u001b[39m     presence_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.presence_penalty),\n\u001b[32m    272\u001b[39m     max_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    273\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    274\u001b[39m     response_format=response_format,\n\u001b[32m    275\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    276\u001b[39m     stream=stream,\n\u001b[32m    277\u001b[39m     stream_options=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(stream_options),\n\u001b[32m    278\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(store),\n\u001b[32m    279\u001b[39m     reasoning_effort=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(reasoning_effort),\n\u001b[32m    280\u001b[39m     extra_headers={**HEADERS, **(model_settings.extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[32m    281\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    282\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    283\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    284\u001b[39m )\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, ChatCompletion):\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/Udemy/projects/agents/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:2028\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1985\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1986\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1987\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2025\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2026\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2027\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2028\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2029\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2030\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2031\u001b[39m             {\n\u001b[32m   2032\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2033\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2034\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2035\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2036\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2037\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2038\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2039\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2040\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2041\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2042\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2043\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2044\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2045\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2046\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2047\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2048\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2049\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2050\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2051\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2052\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2053\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2054\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2055\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2056\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2057\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2058\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2059\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2060\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2061\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2062\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2063\u001b[39m             },\n\u001b[32m   2064\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2065\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2066\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2067\u001b[39m         ),\n\u001b[32m   2068\u001b[39m         options=make_request_options(\n\u001b[32m   2069\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2070\u001b[39m         ),\n\u001b[32m   2071\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2072\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2073\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2074\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/Udemy/projects/agents/.venv/lib/python3.12/site-packages/openai/_base_client.py:1748\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1734\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1735\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1736\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1743\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1744\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1745\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1746\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1747\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1748\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/Udemy/projects/agents/.venv/lib/python3.12/site-packages/openai/_base_client.py:1555\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1552\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1554\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1555\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1557\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '27s'}]}}]"
     ]
    }
   ],
   "source": [
    "# RUN AGENTIC SYSTEM HERE!\n",
    "\n",
    "message = \"Latest AI Agent frameworks in 2025\"\n",
    "\n",
    "with trace(\"Search\"):\n",
    "    final_result = await Runner.run(research_manager_agent, message)\n",
    "    print(\"Report Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(final_result.final_output.short_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(final_result.final_output.markdown_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace\n",
    "\n",
    "https://platform.openai.com/traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.extensions.visualization import draw_graph\n",
    "draw_graph(research_manager_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.extensions.visualization import draw_graph\n",
    "draw_graph(writer_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.extensions.visualization import draw_graph\n",
    "draw_graph(editor_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
